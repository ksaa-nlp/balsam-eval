import json
import os
import time
from typing import List, Dict, Any, Optional, Literal
from dataclasses import dataclass
from statistics import mean, median
from tqdm import tqdm
import requests


# --- Embedded Gemini API Call Function ---
def call_gemini_api(prompt,
                    max_output_tokens=5000, temperature=0, top_p=1.0, top_k=32,
                    max_retries=10):
    model_name = os.getenv("JUDGE_MODEL")
    api_key = os.getenv("JUDGE_API_KEY")

    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={api_key}"
    headers = {"Content-Type": "application/json"}

    payload = {
        "contents": [
            {
                "parts": [{"text": prompt}]
            }
        ],
        "generationConfig": {
            "temperature": temperature,
            "topP": top_p,
            "topK": top_k,
            "maxOutputTokens": max_output_tokens
        }
    }

    for attempt in range(max_retries):
        try:
            response = requests.post(
                url, headers=headers, data=json.dumps(payload))
            if response.status_code == 200:
                result = response.json()

                try:
                    raw_text = result["candidates"][0]["content"]["parts"][0]["text"]
                    raw_text = raw_text.strip().replace("```json", "").replace("```", "").strip()
                    parsed = json.loads(raw_text)

                    if "score" in parsed and "explanation" in parsed:
                        return {
                            "score": parsed["score"],
                            "explanation": parsed["explanation"]
                        }

                    print(
                        f"âš ï¸ Missing keys in parsed output (attempt {attempt+1}): {parsed}")
                except Exception as e:
                    print(
                        f"âš ï¸ JSON parsing or structure error (attempt {attempt+1}): {e}")

            else:
                print(
                    f"âš ï¸ API error (attempt {attempt+1}): {response.status_code}")
                try:
                    print("ðŸ”", response.json())
                except:
                    print("ðŸ§¾", response.text)

        except requests.exceptions.RequestException as e:
            print(f"âŒ Network error (attempt {attempt+1}): {e}")

        wait_time = min(2 ** attempt, 30)
        print(f"â³ Retrying after {wait_time} seconds...")
        time.sleep(wait_time)

    return {
        "score": None,
        "explanation": "Error: Unable to parse a valid response after retries"
    }


# --- Data Classes ---
@dataclass
class EvaluationResult:
    score: float
    raw_score: float
    explanation: str
    passed: bool


@dataclass
class ModelConfig:
    name: str
    provider: Literal["gemini"] = "gemini"
    api_key: Optional[str] = None
    endpoint_url: Optional[str] = None
    other: Optional[Dict[str, Any]] = None


@dataclass
class TestCaseDict:
    question: str
    reference_answer: str
    given_answer: str
    context: Optional[str] = None
    id: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


# --- LLMJudge Class Using Gemini ---
class LLMJudge:
    def __init__(self, model_configs: List[ModelConfig], aggregation_method: str = "mean", threshold: float = 0.7):
        self.model_configs = model_configs
        self.aggregation_method = aggregation_method
        self.threshold = threshold

        assert all(config.provider ==
                   "gemini" for config in model_configs), "Only Gemini provider supported"

        self.evaluation_prompt = self._get_prompt()

    def _get_prompt(self) -> str:
        return '''You are an impartial and expert judge evaluating the quality of text generated by another AI model.
 
Your task is to score the generated output based on the original prompt and a provided ground truth answer, following a specific scoring rubric.
 
You will be provided with three pieces of information:
1. The original prompt given to the generative model.
2. The ground truth answer, representing the ideal or expected output.
3. The actual output generated by the generative model.
 
Evaluate the generated output by comparing it to the ground truth, considering how well it addresses the original prompt.
 
**Scoring Rubric:**
* **Score 0:** The automatically generated output is completely wrong, irrelevant, or unrelated to the prompt and ground truth.
* **Score 1:** Poor answer. The output attempts to address the prompt but contains significant errors, is largely incomplete, or is difficult to understand. It shows little resemblance to the ground truth.
* **Score 2:** Acceptable but different. The output is somewhat correct or addresses parts of the prompt reasonably well, but it differs significantly from the ground truth. It might be missing details present in the ground truth, include extra information not in the ground truth, or present the information in a substantially different structure or style, but is still a valid (though not ideal) response to the prompt.
* **Score 3:** Perfect or almost perfect. The output is accurate, complete, and closely matches the ground truth in content and style, effectively answering the original prompt. Minor differences in wording or formatting that do not affect the meaning or quality are acceptable for a score of 3.
 
**Output Format:**
Your output must be *only* a JSON object containing two keys:
1. `score`: An integer between 0 and 3 based on the rubric above.
2. `explanation`: A brief, concise string explaining *why* you assigned that score, referencing the differences or similarities between the generated output and the ground truth in the context of the prompt.
 
**Example Output JSON:**
```json
{
  "score": 3,
  "explanation": "The generated output is accurate and complete, closely matching the ground truth."
}```'''

    def _evaluate_single_model(self, question, reference_answer, given_answer, context, config) -> Dict[str, Any]:
        prompt = self.evaluation_prompt
        prompt += f'\n\n[PROMPT]\n{question}\n[/PROMPT]\n'
        prompt += f'[GROUND TRUTH]\n{reference_answer}\n[/GROUND TRUTH]\n'
        prompt += f'[GENERATED OUTPUT]\n{given_answer}\n[/GENERATED OUTPUT]'

        result = call_gemini_api(prompt)

        raw_score = result["score"]
        explanation = result["explanation"]
        norm_score = round(raw_score / 3, 4) if raw_score is not None else 0
        passed = norm_score >= self.threshold

        return {
            "model": config.name,
            "provider": config.provider,
            "score": norm_score,
            "raw_score": raw_score,
            "passed": passed,
            "explanation": explanation
        }

    def evaluate_answer(self, question: str, reference_answer: str, given_answer: str, context: Optional[str] = None, test_id: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None):
        model_results = []

        for config in self.model_configs:
            try:
                result = self._evaluate_single_model(
                    question, reference_answer, given_answer, context, config)
            except Exception as e:
                result = {
                    "model": config.name,
                    "provider": config.provider,
                    "score": 0,
                    "raw_score": 0,
                    "passed": False,
                    "explanation": f"Error: {str(e)}"
                }

            model_results.append(result)

        # Aggregation
        agg = self._aggregate_model_results(model_results)

        return {
            "overall_score": agg["overall_score"],
            "overall_raw_score": agg["overall_raw_score"],
            "overall_passed": agg["overall_score"] >= self.threshold,
            "aggregation_method": self.aggregation_method,
            "model_results": model_results,
            "aggregated_explanation": agg["aggregated_explanation"],
            "metadata": {
                "question": question,
                "reference_answer": reference_answer,
                "evaluated_answer": given_answer,
                "context": context,
                "test_id": test_id,
                **(metadata or {})
            }
        }

    def _aggregate_model_results(self, model_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        scores = [res["score"]
                  for res in model_results if res["score"] is not None]
        raw_scores = [res["raw_score"]
                      for res in model_results if res["raw_score"] is not None]

        if not scores:
            return {"overall_score": 0, "overall_raw_score": 0, "aggregated_explanation": "No valid scores"}

        agg_score = median(
            scores) if self.aggregation_method == "median" else mean(scores)
        agg_raw = median(
            raw_scores) if self.aggregation_method == "median" else mean(raw_scores)

        explanation = f"Aggregated ({self.aggregation_method}) score: {agg_score:.4f}. " + "; ".join(
            f"{res['model']}: {res['explanation']}" for res in model_results
        )

        return {
            "overall_score": agg_score,
            "overall_raw_score": agg_raw,
            "aggregated_explanation": explanation
        }

    def evaluate_batch(self, test_cases: List[TestCaseDict], show_progress=True):
        results = []
        iterator = tqdm(test_cases, desc="Evaluating",
                        unit="case") if show_progress else test_cases

        for tc in iterator:
            result = self.evaluate_answer(
                question=tc.question,
                reference_answer=tc.reference_answer,
                given_answer=tc.given_answer,
                context=tc.context,
                test_id=tc.id,
                metadata=tc.metadata
            )
            results.append(result)

        return {
            "individual_results": results,
            "batch_statistics": self._calculate_batch_statistics(results)
        }

    def _calculate_batch_statistics(self, results: List[Dict[str, Any]]):
        scores = [r["overall_score"] for r in results]
        raw_scores = [r["overall_raw_score"] for r in results]

        return {
            "total_test_cases": len(results),
            "average_score": mean(scores),
            "median_score": median(scores),
            "average_raw_score": mean(raw_scores),
            "median_raw_score": median(raw_scores),
            "pass_rate": sum(1 for s in scores if s >= self.threshold) / len(scores)
        }

    def save_results(self, results: Dict[str, Any], filename: str) -> None:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"âœ… Results saved to {filename}")
