from src.llm_judger.base_llm_judge import BaseLLMJudge


class MCQLLMJudge(BaseLLMJudge):
    """
    LLM Judge specialized for Multiple Choice Questions.
    
    Key differences from generative:
    - Uses binary 0-1 scoring (correct/incorrect)
    - No normalization needed (score is already 0-1)
    - Simpler prompt focused on exact matching
    - Handles Arabic-English letter equivalence (A=أ, B=ب, etc.)
    """

    def get_evaluation_prompt(self) -> str:
        """Get MCQ-specific evaluation prompt with binary 0-1 scoring."""
        return """You are an impartial and expert evaluator for a multiple-choice question.

You will be given two pieces of information:
1. The gold (correct) answer.
2. The answer generated by the model.

Your task is to determine whether the generated answer is correct.

**Evaluation Rules:**
- Assign a score of **1** if any of the following conditions hold:
  - The generated answer matches the gold answer **by letter**, including **Arabic–English equivalence** (e.g., "A" = "أ", "B" = "ب", "C" = "ج", "D" = "د", "E" = "هـ", "F" = "و").
  - The generated answer matches the gold answer **by choice text** (e.g., both are "1/4").
  - The answers correspond to **yes/no equivalents**, where ("yes" = "نعم") and ("no" = "لا").
- Otherwise, assign a score of **0**.

Ignore differences in capitalization, spacing, punctuation, and script (Arabic or Latin).

**Output Format:**
Return only a JSON object with:
1. `score`: 1 if correct, 0 if incorrect.
2. `explanation`: A concise reason explaining why the answer is correct or incorrect.

**Example Output JSON:**

{
  "score": 1,
  "explanation": "The generated answer matches the correct choice considering Arabic-English and yes/no equivalences."
}"""

    def normalize_score(self, raw_score: float) -> float:
        """
        For MCQ, score is already 0-1, no normalization needed.
        Just ensure it's within valid range.
        """
        if raw_score is None:
            return 0.0
        # Clamp to 0-1 range
        return max(0.0, min(1.0, float(raw_score)))

    def get_max_score(self) -> float:
        """MCQ uses binary scoring, max is 1."""
        return 1.0

