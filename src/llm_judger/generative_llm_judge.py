
from src.llm_judger.base_llm_judge import BaseLLMJudge


class GenerativeLLMJudge(BaseLLMJudge):
    """
    LLM Judge specialized for Generative (open-ended) tasks.
    
    Key differences from MCQ:
    - Uses 0-3 scoring scale for nuanced evaluation
    - Normalizes to 0-1 by dividing by 3
    - More detailed rubric for partial credit
    - Focuses on semantic similarity, not exact matching
    """

    def get_evaluation_prompt(self) -> str:
        """Get generative-specific evaluation prompt with 0-3 scoring."""
        return """You are an impartial and expert judge evaluating the quality of text generated by another AI model.

Your task is to score the generated output based on the original prompt and a provided ground truth answer, following a specific scoring rubric.

You will be provided with three pieces of information:
1. The original prompt given to the generative model.
2. The ground truth answer, representing the ideal or expected output.
3. The actual output generated by the generative model.

Evaluate the generated output by comparing it to the ground truth, considering how well it addresses the original prompt.

**Scoring Rubric:**
* **Score 0:** The automatically generated output is completely wrong, irrelevant, or unrelated to the prompt and ground truth.
* **Score 1:** Poor answer. The output attempts to address the prompt but contains significant errors, is largely incomplete, or is difficult to understand. It shows little resemblance to the ground truth.
* **Score 2:** Acceptable but different. The output is somewhat correct or addresses parts of the prompt reasonably well, but it differs significantly from the ground truth. It might be missing details present in the ground truth, include extra information not in the ground truth, or present the information in a substantially different structure or style, but is still a valid (though not ideal) response to the prompt.
* **Score 3:** Perfect or almost perfect. The output is accurate, complete, and closely matches the ground truth in content and style, effectively answering the original prompt. Minor differences in wording or formatting that do not affect the meaning or quality are acceptable for a score of 3.

**Output Format:**
Your output must be *only* a JSON object containing two keys:
1. `score`: An integer between 0 and 3 based on the rubric above.
2. `explanation`: A brief, concise string explaining *why* you assigned that score, referencing the differences or similarities between the generated output and the ground truth in the context of the prompt.

**Example Output JSON:**
```json
{
  "score": 3,
  "explanation": "The generated output is accurate and complete, closely matching the ground truth."
}```"""

    def normalize_score(self, raw_score: float) -> float:
        """
        Normalize score from 0-3 scale to 0-1 scale.
        """
        if raw_score is None:
            return 0.0
        # Normalize by dividing by max score (3)
        return round(float(raw_score) / 3.0, 5)

    def get_max_score(self) -> float:
        """Generative uses 0-3 scoring scale."""
        return 3.0
